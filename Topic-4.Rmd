# Topic 4 Exercises: Classification

# Madeline Olson

# Programming

# 4.7.11

```{r}
library(ISLR)
```

# a

```{r}
mpg01=rep(0, length(Auto$mpg))
mpg01[Auto$mpg>median(Auto$mpg)]=1
Auto=data.frame(Auto, mpg01)


```

# b

```{r}

pairs(Auto)
par(mfrow=c(2,2))
boxplot(Auto$cylinders~Auto$mpg01)
boxplot(Auto$displacement~Auto$mpg01)
boxplot(Auto$horsepower~Auto$mpg01)
boxplot(Auto$weight~Auto$mpg01)
boxplot(Auto$acceleration~Auto$mpg01)
boxplot(Auto$year~Auto$mpg01)
boxplot(Auto$origin~Auto$mpg01)
```

#
Based on this data cylinders, displacement, horsepower, and weight seem to have some effect on mpg01.

# c

```{r}
train=(Auto$year<77)
test=!train
Auto.test=Auto[test,]
Auto.train=Auto[train,]
mpg01.test=Auto$mpg01[test]
dim(Auto.test)
```

# d

```{r}
library(MASS)
lda.fit=lda(mpg01 ~ cylinders+displacement+horsepower+weight,data=Auto, subset=train)
lda.pred=predict(lda.fit, Auto.test)
lda.class=lda.pred$class
mean(lda.class == mpg01.test)
1-mean(lda.class == mpg01.test)
```

# e

```{r}
qda.fit=qda(mpg01 ~ cylinders+displacement+horsepower+weight,data=Auto, subset=train)
qda.class=predict(qda.fit, Auto.test)$class
mean(qda.class==mpg01.test)
1-mean(qda.class==mpg01.test)
```

# f

```{r}

glm.fit=glm(mpg01 ~ cylinders+displacement+horsepower+weight, data=Auto, subset=train)
dim(Auto.test)
glm.probs=predict(glm.fit, Auto.test, type="response")

glm.pred=rep(0,181)
glm.pred[glm.probs>.5]=1

mean(glm.pred != mpg01.test)

```

# g

```{r}
library(class)
train.X=cbind(Auto$cylinders, Auto$weight, Auto$displacement, Auto$horsepower)[train,]
test.X=cbind(Auto$cylinders, Auto$weight, Auto$displacement, Auto$horsepower)[!train,]
train.mpg01=Auto$mpg01[train]
set.seed(1)
knn.pred=knn(train.X, test.X, train.mpg01, k=1)
table(knn.pred, mpg01.test)
mean(knn.pred!=mpg01.test)

knn.pred=knn(train.X, test.X, train.mpg01, k=5)
table(knn.pred, mpg01.test)
mean(knn.pred!=mpg01.test)

knn.pred=knn(train.X, test.X, train.mpg01, k=10)
table(knn.pred, mpg01.test)
mean(knn.pred!=mpg01.test)

knn.pred=knn(train.X, test.X, train.mpg01, k=30)
table(knn.pred, mpg01.test)
mean(knn.pred!=mpg01.test)
```

#
k=1 has the lowest test error

# 4.7.13

```{r}
library(MASS)
summary(Boston)
crime01=rep(0, length(Boston$crim))
crime01[Boston$crim>median(Boston$crim)]=1
Boston=data.frame(Boston, crime01)
```
```{r}
pairs(~zn+indus+chas+nox+rm+age+dis+crime01,data=Boston)
pairs(~rad+tax+ptratio+black+lstat+medv+crime01,data=Boston)
par(mfrow=c(2,2))
boxplot(Boston$zn~Boston$crime01)
boxplot(Boston$indus~Boston$crime01)
boxplot(Boston$nox~Boston$crime01)
boxplot(Boston$rm~Boston$crime01)
boxplot(Boston$age~Boston$crime01)
boxplot(Boston$dis~Boston$crime01)
boxplot(Boston$rad~Boston$crime01)
boxplot(Boston$tax~Boston$crime01)
boxplot(Boston$ptratio~Boston$crime01)
boxplot(Boston$black~Boston$crime01)
boxplot(Boston$lstat~Boston$crime01)
boxplot(Boston$medv~Boston$crime01)
```

```{r}
train=1:253
test=254:506
Boston.train=Boston[train,]
Boston.test=Boston[test,]
crime01.test=crime01[test]
```


```{r}
library(MASS)
lda.fit=lda(crime01 ~ indus+nox+tax,data=Boston, subset=train)
lda.pred=predict(lda.fit, Boston.test)
lda.class=lda.pred$class
mean(lda.class == crime01.test)
print("lda error")
1-mean(lda.class == crime01.test)

qda.fit=qda(crime01 ~ indus+nox+tax,data=Boston, subset=train)
qda.class=predict(qda.fit, Boston.test)$class
mean(qda.class==crime01.test)
print("qda error")
1-mean(qda.class==crime01.test)
```

```{r}
glm.fit=glm(crime01 ~ indus+nox+tax,data=Boston, subset=train)
dim(Boston.test)
glm.probs=predict(glm.fit, Boston.test, type="response")

glm.pred=rep(0,253)
glm.pred[glm.probs>.5]=1

mean(glm.pred != crime01.test)
```


```{r}
library(class)
train.X=cbind(Boston$indus,Boston$nox,Boston$tax)[train,]
test.X=cbind(Boston$indus,Boston$nox,Boston$tax)[test,]
train.crime01=Boston$crime01[train]
set.seed(1)
knn.pred=knn(train.X, test.X, train.crime01, k=1)
table(knn.pred, crime01.test)
mean(knn.pred!=crime01.test)

knn.pred=knn(train.X, test.X, train.crime01, k=10)
table(knn.pred, crime01.test)
mean(knn.pred!=crime01.test)

knn.pred=knn(train.X, test.X, train.crime01, k=50)
table(knn.pred, crime01.test)
mean(knn.pred!=crime01.test)

```

#
When selecting indus, nox, and tax as our associated predictors LDA and logistic regesssion have the lowest test errors.

# Theory

# 4.7.1

#
If we substitute A for e^B0+B1X then (4.2) is p(X)=A/1+A and (4.3) is p(X)/1-p(X)=A. We can then substitute A/1+A for p(X) in (4.3), making it (A/1+A)/1-(A/1+A) which can be rewritten as (A/1+A)*(1/(1-(A/1+A)). Then by multiplying by (1+A/1+A) we get A/(1+A)*(1+A)/(1+A-A) which simplifies to A.

# 4.7.8

#
While the average test score for the logistic regression is higher than the nearest neighbors method. It is very likely this low average is due to a low training error. Since it is 1-nearest neighboor test error is likely to be around 1% meaning the test error is near 36%. Because this method often over fits the data it is more likely to fail when applied than less flexible methods.

# 4.7.9

# a
.37=1.37*p(X)
p(X)=0.270
27 of 100 people

# b
.16/(1-.16)=0.190
